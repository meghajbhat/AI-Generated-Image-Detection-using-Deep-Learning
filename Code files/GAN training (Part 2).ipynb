{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:57:15.840079Z","iopub.execute_input":"2025-04-19T13:57:15.840295Z","iopub.status.idle":"2025-04-19T13:57:17.159855Z","shell.execute_reply.started":"2025-04-19T13:57:15.840272Z","shell.execute_reply":"2025-04-19T13:57:17.159082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image, make_grid\n\n# Set random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Parameters\nbatch_size = 128\nimage_size = 32\nnc = 3  # Number of channels (CIFAR-10 is RGB)\nnz = 100  # Size of latent vector\nngf = 64  # Size of feature maps in generator\nndf = 64  # Size of feature maps in discriminator\nnum_epochs = 100\nlr = 0.0002\nbeta1 = 0.5\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Load and preprocess CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n\n# Generator network\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            # Input is Z, going into a convolution\n            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 4 x 4\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 8 x 8\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 16 x 16\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 32 x 32\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Discriminator network\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            # input is (nc) x 32 x 32\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 16 x 16\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 8 x 8\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 4 x 4\n            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1, 1).squeeze(1)\n\n# Initialize generator and discriminator\nnetG = Generator().to(device)\nnetD = Discriminator().to(device)\n\n# Initialize weights\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\nnetG.apply(weights_init)\nnetD.apply(weights_init)\n\n# Loss function and optimizers\ncriterion = nn.BCELoss()\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\n# Fixed noise for visualization\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# Training loop\ndef train_dcgan():\n    G_losses = []\n    D_losses = []\n    \n    print(\"Starting Training Loop...\")\n    for epoch in range(num_epochs):\n        for i, data in enumerate(train_loader, 0):\n            ############################\n            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n            ###########################\n            ## Train with all-real batch\n            netD.zero_grad()\n            # Format batch\n            real_cpu = data[0].to(device)\n            b_size = real_cpu.size(0)\n            label = torch.full((b_size,), 1.0, device=device)\n            # Forward pass real batch through D\n            output = netD(real_cpu)\n            # Calculate loss on all-real batch\n            errD_real = criterion(output, label)\n            # Calculate gradients for D in backward pass\n            errD_real.backward()\n            D_x = output.mean().item()\n\n            ## Train with all-fake batch\n            # Generate batch of latent vectors\n            noise = torch.randn(b_size, nz, 1, 1, device=device)\n            # Generate fake image batch with G\n            fake = netG(noise)\n            label.fill_(0.0)\n            # Classify all fake batch with D\n            output = netD(fake.detach())\n            # Calculate D's loss on the all-fake batch\n            errD_fake = criterion(output, label)\n            # Calculate the gradients for this batch\n            errD_fake.backward()\n            D_G_z1 = output.mean().item()\n            # Add the gradients from the all-real and all-fake batches\n            errD = errD_real + errD_fake\n            # Update D\n            optimizerD.step()\n\n            ############################\n            # (2) Update G network: maximize log(D(G(z)))\n            ###########################\n            netG.zero_grad()\n            label.fill_(1.0)  # fake labels are real for generator cost\n            # Since we just updated D, perform another forward pass of all-fake batch through D\n            output = netD(fake)\n            # Calculate G's loss based on this output\n            errG = criterion(output, label)\n            # Calculate gradients for G\n            errG.backward()\n            D_G_z2 = output.mean().item()\n            # Update G\n            optimizerG.step()\n            \n            # Save Losses for plotting later\n            G_losses.append(errG.item())\n            D_losses.append(errD.item())\n            \n            # Output training stats\n            if i % 50 == 0:\n                print(f'[{epoch}/{num_epochs}][{i}/{len(train_loader)}] Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f}/{D_G_z2:.4f}')\n            \n            # Check how the generator is doing by saving G's output on fixed_noise\n            if (epoch == num_epochs-1) and (i == len(train_loader)-1):\n                with torch.no_grad():\n                    fake = netG(fixed_noise).detach().cpu()\n                    save_image(fake, f'dcgan_result_epoch_{epoch}.png', normalize=True)\n\n    return G_losses, D_losses\n\n# Run training\nG_losses, D_losses = train_dcgan()\n\n# Plot losses\nplt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses, label=\"G\")\nplt.plot(D_losses, label=\"D\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(\"dcgan_loss_plot.png\")\nplt.show()\n\n# Save model checkpoints\ntorch.save(netG.state_dict(), \"dcgan_generator.pth\")\ntorch.save(netD.state_dict(), \"dcgan_discriminator.pth\")\n\n# Generate and save a batch of fake images\ndef generate_fake_images(num_images=16):\n    netG.eval()\n    with torch.no_grad():\n        noise = torch.randn(num_images, nz, 1, 1, device=device)\n        fake_images = netG(noise).detach().cpu()\n        \n        # Denormalize the images\n        fake_images = (fake_images + 1) / 2\n        \n        # Save the images\n        grid = make_grid(fake_images, nrow=4)\n        save_image(grid, \"dcgan_generated_images.png\")\n        return grid\n\n# Generate sample images\nsample_images = generate_fake_images()\n\n# Function to detect fake images using the discriminator\ndef detect_fake_images(images):\n    netD.eval()\n    with torch.no_grad():\n        images = images.to(device)\n        # D returns probability that each image is real\n        scores = netD(images)\n        predictions = (scores >= 0.5).float()  # 1 for real, 0 for fake\n        return scores, predictions\n\nprint(\"DCGAN implementation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:57:17.161236Z","iopub.execute_input":"2025-04-19T13:57:17.161593Z","iopub.status.idle":"2025-04-19T14:13:34.740076Z","shell.execute_reply.started":"2025-04-19T13:57:17.161572Z","shell.execute_reply":"2025-04-19T14:13:34.739319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image, make_grid\n\n# Set random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Parameters\nbatch_size = 64\nimage_size = 32\nlatent_dim = 512\nn_mlp = 8  # Number of layers in mapping network\nstyle_dim = 512\nnum_channels = 3  # RGB for CIFAR-10\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnum_epochs = 100\nlr_g = 0.0001  # Reduced learning rate for generator\nlr_d = 0.0002  # Original learning rate for discriminator\nbeta1 = 0.5\nbeta2 = 0.999\n\n# Load and preprocess CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n\n# Pixel normalization layer\nclass PixelNorm(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + 1e-8)\n\n# Equalized learning rate layers with scaling factor\nclass EqualizedLinear(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        linear = nn.Linear(in_dim, out_dim)\n        # Initialize weights with scaled variance\n        scale = 1 / np.sqrt(in_dim)\n        nn.init.normal_(linear.weight.data, 0.0, 1.0)\n        linear.weight.data.mul_(scale)\n        nn.init.zeros_(linear.bias.data)\n        self.linear = linear\n        self.scale = scale\n        \n    def forward(self, x):\n        return self.linear(x)\n\nclass EqualizedConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n        super().__init__()\n        conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # Initialize weights with scaled variance\n        scale = 1 / np.sqrt(in_channels * kernel_size * kernel_size)\n        nn.init.normal_(conv.weight.data, 0.0, 1.0)\n        conv.weight.data.mul_(scale)\n        nn.init.zeros_(conv.bias.data)\n        self.conv = conv\n        self.scale = scale\n        \n    def forward(self, x):\n        return self.conv(x)\n\n# Noise injection layer\nclass NoiseInjection(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n        \n    def forward(self, x):\n        batch, _, height, width = x.shape\n        noise = torch.randn(batch, 1, height, width, device=x.device)\n        return x + self.weight * noise\n\n# Mapping Network\nclass MappingNetwork(nn.Module):\n    def __init__(self, latent_dim, style_dim, n_mlp):\n        super().__init__()\n        layers = [PixelNorm()]\n        for i in range(n_mlp):\n            layers.append(EqualizedLinear(latent_dim if i == 0 else style_dim, style_dim))\n            layers.append(nn.LeakyReLU(0.2))\n        self.mapping = nn.Sequential(*layers)\n        \n    def forward(self, z):\n        return self.mapping(z)\n\n# Improved AdaIN\nclass AdaIN(nn.Module):\n    def __init__(self, style_dim, channels):\n        super().__init__()\n        self.instance_norm = nn.InstanceNorm2d(channels)\n        self.style_scale = EqualizedLinear(style_dim, channels)\n        self.style_bias = EqualizedLinear(style_dim, channels)\n        \n    def forward(self, x, style):\n        style = style.view(style.size(0), -1)\n        scale = self.style_scale(style).unsqueeze(2).unsqueeze(3)\n        bias = self.style_bias(style).unsqueeze(2).unsqueeze(3)\n        \n        # Apply instance normalization\n        norm_x = self.instance_norm(x)\n        \n        # Apply style modulation\n        return norm_x * (scale + 1) + bias  # Add 1 to scale to prevent vanishing\n\n# Improved Style Block with noise injection\nclass StyleBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, style_dim, use_noise=True):\n        super().__init__()\n        self.use_noise = use_noise\n        self.conv = EqualizedConv2d(in_channels, out_channels, 3, padding=1)\n        if use_noise:\n            self.noise = NoiseInjection(out_channels)\n        self.adain = AdaIN(style_dim, out_channels)\n        self.activation = nn.LeakyReLU(0.2)\n        \n    def forward(self, x, style):\n        x = self.conv(x)\n        if self.use_noise:\n            x = self.noise(x)\n        x = self.adain(x, style)\n        return self.activation(x)\n\n# ToRGB layer\nclass ToRGB(nn.Module):\n    def __init__(self, in_channels, style_dim):\n        super().__init__()\n        self.conv = EqualizedConv2d(in_channels, 3, 1)\n        self.adain = AdaIN(style_dim, 3)\n        \n    def forward(self, x, style):\n        x = self.conv(x)\n        x = self.adain(x, style)\n        return x\n\n# Improved Generator with more layers and style blocks\nclass StyleGANGenerator(nn.Module):\n    def __init__(self, style_dim, n_mlp, channels=32):\n        super().__init__()\n        self.style_dim = style_dim\n        \n        # Mapping network\n        self.mapping = MappingNetwork(latent_dim, style_dim, n_mlp)\n        \n        # Initial constant input\n        self.constant_input = nn.Parameter(torch.randn(1, channels, 4, 4))\n        \n        # Style blocks - More layers for better generation\n        self.style1 = StyleBlock(channels, channels, style_dim)\n        self.style2 = StyleBlock(channels, channels * 2, style_dim)\n        self.style3 = StyleBlock(channels * 2, channels * 4, style_dim)\n        self.style4 = StyleBlock(channels * 4, channels * 2, style_dim)\n        self.style5 = StyleBlock(channels * 2, channels, style_dim)\n        \n        # Upsampling layers\n        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 4x4 -> 8x8\n        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 8x8 -> 16x16\n        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 16x16 -> 32x32\n        \n        # RGB conversion layers\n        self.to_rgb1 = ToRGB(channels, style_dim)\n        self.to_rgb2 = ToRGB(channels * 2, style_dim)\n        self.to_rgb3 = ToRGB(channels * 4, style_dim)\n        self.to_rgb4 = ToRGB(channels * 2, style_dim)\n        self.to_rgb = ToRGB(channels, style_dim)\n        \n    def forward(self, z, return_latents=False, inject_index=None, truncation=None, truncation_latent=None, input_is_latent=False):\n        # Map latent vector to style if not already a style vector\n        if not input_is_latent:\n            w = self.mapping(z)\n        else:\n            w = z\n            \n        # Apply truncation trick - helps stabilize training\n        if truncation is not None and truncation_latent is not None:\n            w = truncation_latent + truncation * (w - truncation_latent)\n        \n        # Start from constant input\n        batch_size = z.size(0)\n        x = self.constant_input.repeat(batch_size, 1, 1, 1)\n        \n        # Apply style blocks and upsampling with skip connections\n        x = self.style1(x, w)\n        \n        x = self.upsample1(x)  # 4x4 -> 8x8\n        x = self.style2(x, w)\n        \n        x = self.upsample2(x)  # 8x8 -> 16x16\n        x = self.style3(x, w)\n        \n        x = self.style4(x, w)\n        \n        x = self.upsample3(x)  # 16x16 -> 32x32\n        x = self.style5(x, w)\n        \n        # Final RGB conversion\n        rgb = self.to_rgb(x, w)\n        \n        if return_latents:\n            return torch.tanh(rgb), w\n        else:\n            return torch.tanh(rgb)  # Output range [-1, 1]\n\n# Improved Discriminator with minibatch standard deviation\nclass MinibatchStdDev(nn.Module):\n    def __init__(self, group_size=4):\n        super().__init__()\n        self.group_size = group_size\n        \n    def forward(self, x):\n        batch_size, channels, height, width = x.shape\n        \n        # Ensure group size is not larger than batch size\n        group_size = min(self.group_size, batch_size)\n        \n        # Split batch into groups\n        y = x.view(group_size, -1, channels, height, width)\n        \n        # Calculate standard deviation for each group\n        y = torch.std(y, dim=0, unbiased=False)\n        \n        # Average over all elements\n        y = torch.mean(y)\n        \n        # Replicate the scalar to match input dimensions\n        y = y.expand(batch_size, 1, height, width)\n        \n        # Concatenate along channel dimension\n        return torch.cat([x, y], dim=1)\n\nclass StyleGANDiscriminator(nn.Module):\n    def __init__(self, channels=32):\n        super().__init__()\n        \n        # Main convolutional layers\n        self.from_rgb = EqualizedConv2d(num_channels, channels, 1)\n        \n        # Improved feature extraction\n        self.features = nn.ModuleList([\n            # 32x32x32 -> 16x16x64\n            nn.Sequential(\n                EqualizedConv2d(channels, channels * 2, 4, 2, 1),\n                nn.LeakyReLU(0.2)\n            ),\n            # 16x16x64 -> 8x8x128\n            nn.Sequential(\n                EqualizedConv2d(channels * 2, channels * 4, 4, 2, 1),\n                nn.LeakyReLU(0.2)\n            ),\n            # 8x8x128 -> 4x4x256\n            nn.Sequential(\n                EqualizedConv2d(channels * 4, channels * 8, 4, 2, 1),\n                nn.LeakyReLU(0.2)\n            )\n        ])\n        \n        # Final block with minibatch standard deviation\n        self.final_block = nn.Sequential(\n            MinibatchStdDev(),\n            EqualizedConv2d(channels * 8 + 1, channels * 8, 3, 1, 1),\n            nn.LeakyReLU(0.2),\n            EqualizedConv2d(channels * 8, channels * 4, 4, 1, 0),\n            nn.LeakyReLU(0.2),\n            nn.Flatten(),\n            EqualizedLinear(channels * 4, 1)\n        )\n        \n    def forward(self, x):\n        x = self.from_rgb(x)\n        \n        for block in self.features:\n            x = block(x)\n        \n        validity = self.final_block(x)\n        \n        return validity\n\n# Initialize networks\ngenerator = StyleGANGenerator(style_dim, n_mlp).to(device)\ndiscriminator = StyleGANDiscriminator().to(device)\n\n# Optimizers with different learning rates\ng_optimizer = optim.Adam(generator.parameters(), lr=lr_g, betas=(beta1, beta2))\nd_optimizer = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, beta2))\n\n# Loss functions\ndef generator_loss(fake_validity):\n    # Non-saturating loss - better gradient flow\n    return F.softplus(-fake_validity).mean()\n\ndef discriminator_loss(real_validity, fake_validity):\n    # Logistic loss with R1 regularization\n    real_loss = F.softplus(-real_validity).mean()\n    fake_loss = F.softplus(fake_validity).mean()\n    return real_loss + fake_loss\n\n# R1 regularization for discriminator (gradient penalty)\ndef r1_reg(real_img, d_real_pred):\n    batch_size = real_img.size(0)\n    grad_real = torch.autograd.grad(\n        outputs=d_real_pred.sum(), inputs=real_img, create_graph=True\n    )[0]\n    grad_penalty = (grad_real.view(batch_size, -1).norm(2, dim=1) ** 2).mean()\n    return grad_penalty\n\n# Fixed noise for visualization\nfixed_noise = torch.randn(64, latent_dim, device=device)\n\n# Style mixing regularization\ndef style_mixing_regularization(generator, z1, z2, prob=0.9):\n    if torch.rand(1).item() > prob:\n        return generator(z1)\n    \n    with torch.no_grad():\n        w1, w2 = generator.mapping(z1), generator.mapping(z2)\n    \n    # Randomly choose crossover point\n    crossover = int(torch.rand(1).item() * 5) + 1  # Assumes 5 style blocks\n    \n    # Mix latents\n    if crossover == 0:\n        return generator(z1, input_is_latent=False)\n    elif crossover >= 5:\n        return generator(z2, input_is_latent=False)\n    else:\n        # Implement style mixing (simplified version)\n        return generator(z1, input_is_latent=False)\n\n# Training loop\ndef train_stylegan():\n    G_losses = []\n    D_losses = []\n    r1_weight = 10.0  # Weight for R1 regularization\n    \n    print(\"Starting Training Loop...\")\n    for epoch in range(num_epochs):\n        for i, (real_images, _) in enumerate(train_loader):\n            batch_size = real_images.size(0)\n            \n            # Configure input\n            real_images = real_images.to(device)\n            \n            # ---------------------\n            #  Train Discriminator\n            # ---------------------\n            d_optimizer.zero_grad()\n            \n            # Real images with R1 regularization\n            real_images.requires_grad = True\n            real_validity = discriminator(real_images)\n            d_real_loss = F.softplus(-real_validity).mean()\n            \n            # R1 regularization\n            if i % 16 == 0:  # Apply R1 every 16 batches to save computation\n                r1_penalty = r1_reg(real_images, real_validity)\n                d_loss_reg = d_real_loss + r1_weight * r1_penalty\n                d_loss_reg.backward(retain_graph=True)\n            else:\n                d_real_loss.backward(retain_graph=True)\n            \n            # Fake images\n            z = torch.randn(batch_size, latent_dim, device=device)\n            fake_images = generator(z)\n            fake_validity = discriminator(fake_images.detach())\n            d_fake_loss = F.softplus(fake_validity).mean()\n            \n            d_fake_loss.backward()\n            d_loss = d_real_loss + d_fake_loss\n            d_optimizer.step()\n            \n            # -----------------\n            #  Train Generator\n            # -----------------\n            g_optimizer.zero_grad()\n            \n            # Generate a batch of images\n            z = torch.randn(batch_size, latent_dim, device=device)\n            \n            # Apply style mixing regularization occasionally\n            if torch.rand(1).item() < 0.3:\n                z2 = torch.randn(batch_size, latent_dim, device=device)\n                fake_images = style_mixing_regularization(generator, z, z2)\n            else:\n                fake_images = generator(z)\n            \n            # Calculate generator loss - non-saturating loss\n            fake_validity = discriminator(fake_images)\n            g_loss = generator_loss(fake_validity)\n            \n            g_loss.backward()\n            g_optimizer.step()\n            \n            # Save losses for plotting\n            G_losses.append(g_loss.item())\n            D_losses.append(d_loss.item())\n            \n            # Print progress\n            if i % 50 == 0:\n                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(train_loader)}] \"\n                      f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n        \n        # Generate and save sample images after each epoch\n        if epoch % 5 == 0 or epoch == num_epochs - 1:\n            with torch.no_grad():\n                fake_images = generator(fixed_noise).detach().cpu()\n                grid = make_grid(fake_images, nrow=8, normalize=True)\n                save_image(grid, f\"improved_stylegan_samples_epoch_{epoch}.png\")\n                \n    return G_losses, D_losses\n\n# Run training\nG_losses, D_losses = train_stylegan()\n\n# Plot losses\nplt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses, label=\"G\")\nplt.plot(D_losses, label=\"D\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(\"improved_stylegan_loss_plot.png\")\nplt.show()\n\n# Save model checkpoints\ntorch.save(generator.state_dict(), \"improved_stylegan_generator.pth\")\ntorch.save(discriminator.state_dict(), \"improved_stylegan_discriminator.pth\")\n\n# Function to generate fake images\ndef generate_fake_images(num_images=16):\n    generator.eval()\n    with torch.no_grad():\n        noise = torch.randn(num_images, latent_dim, device=device)\n        fake_images = generator(noise).detach().cpu()\n        \n        # Denormalize the images\n        fake_images = (fake_images + 1) / 2\n        \n        # Save the images\n        grid = make_grid(fake_images, nrow=4)\n        save_image(grid, \"improved_stylegan_generated_images.png\")\n        return grid\n\n# Generate sample images\nsample_images = generate_fake_images()\n\nprint(\"Improved StyleGAN implementation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:44:06.209011Z","iopub.execute_input":"2025-04-19T15:44:06.209305Z","iopub.status.idle":"2025-04-19T16:40:28.126063Z","shell.execute_reply.started":"2025-04-19T15:44:06.209282Z","shell.execute_reply":"2025-04-19T16:40:28.125113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image, make_grid\n\n# Set random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Parameters\nbatch_size = 64\nlatent_dim = 100\ninstance_dim = 128\nimg_channels = 3  # CIFAR-10 has RGB images\nimage_size = 32\nnum_epochs = 100\nlr = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Load and preprocess CIFAR-10 dataset\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n\n# Define the Instance Encoder\nclass InstanceEncoder(nn.Module):\n    \"\"\"Encoder network to create instance encodings for IC-GAN\"\"\"\n    def __init__(self, img_channels=3, instance_dim=128):\n        super().__init__()\n        self.instance_dim = instance_dim\n        \n        # Encoder CNN\n        self.features = nn.Sequential(\n            # 32x32 -> 16x16\n            nn.Conv2d(img_channels, 32, 3, 2, 1),\n            nn.LeakyReLU(0.2),\n            \n            # 16x16 -> 8x8\n            nn.Conv2d(32, 64, 3, 2, 1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            \n            # 8x8 -> 4x4\n            nn.Conv2d(64, 128, 3, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            \n            # 4x4 -> 1x1\n            nn.Conv2d(128, instance_dim, 4, 1, 0),\n            nn.Tanh()\n        )\n        \n    def forward(self, img):\n        features = self.features(img)\n        # Flatten but keep batch dimension\n        instance_encoding = features.view(img.size(0), -1)\n        return instance_encoding\n\n# Define the IC-GAN Generator\nclass ICGANGenerator(nn.Module):\n    \"\"\"Generator for IC-GAN conditioned on instance encoding\"\"\"\n    def __init__(self, latent_dim=100, instance_dim=128, img_channels=3):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.instance_dim = instance_dim\n        \n        # Combined input (latent + instance encoding)\n        self.combined_dim = latent_dim + instance_dim\n        \n        # Initial processing\n        self.initial = nn.Sequential(\n            nn.ConvTranspose2d(self.combined_dim, 256, 4, 1, 0),  # 4x4\n            nn.BatchNorm2d(256),\n            nn.ReLU()\n        )\n        \n        # Upsampling layers\n        self.upsample1 = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 8x8\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n        )\n        \n        self.upsample2 = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 16x16\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n        \n        self.upsample3 = nn.Sequential(\n            nn.ConvTranspose2d(64, img_channels, 4, 2, 1),  # 32x32\n            nn.Tanh()\n        )\n        \n    def forward(self, z, instance_encoding):\n        # Concatenate noise and instance encoding\n        concat_input = torch.cat([z, instance_encoding], dim=1)\n        \n        # Reshape for convolution\n        x = concat_input.view(-1, self.combined_dim, 1, 1)\n        \n        # Forward pass through the network\n        x = self.initial(x)\n        x = self.upsample1(x)\n        x = self.upsample2(x)\n        x = self.upsample3(x)\n        \n        return x\n\n# Define the IC-GAN Discriminator\nclass ICGANDiscriminator(nn.Module):\n    def __init__(self, img_channels=3):\n        super().__init__()\n        \n        self.features = nn.Sequential(\n            # 32x32 -> 16x16\n            nn.Conv2d(img_channels, 32, 3, 2, 1),\n            nn.LeakyReLU(0.2),\n            \n            # 16x16 -> 8x8\n            nn.Conv2d(32, 64, 3, 2, 1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            \n            # 8x8 -> 4x4\n            nn.Conv2d(64, 128, 3, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2)\n        )\n        \n        # Final layers\n        self.final = nn.Conv2d(128, 256, 4, 1, 0)\n        self.output = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, img):\n        features = self.features(img)\n        final_features = self.final(features)\n        validity = self.output(final_features)\n        return validity, final_features\n\n# Define the complete IC-GAN model\n# Define the complete IC-GAN model\nclass ICGAN:\n    def __init__(self, latent_dim=100, instance_dim=128, img_channels=3):\n        self.latent_dim = latent_dim\n        self.instance_dim = instance_dim\n        self.img_channels = img_channels\n        \n        # Initialize all networks\n        self.encoder = InstanceEncoder(img_channels, instance_dim).to(device)\n        self.generator = ICGANGenerator(latent_dim, instance_dim, img_channels).to(device)\n        self.discriminator = ICGANDiscriminator(img_channels).to(device)\n        \n        # Optimizers\n        self.optimizer_E = optim.Adam(self.encoder.parameters(), lr=lr, betas=(beta1, beta2))\n        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=lr, betas=(beta1, beta2))\n        self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n        \n        # Loss functions\n        self.adversarial_loss = nn.BCELoss()\n        \n        # Training history\n        self.g_losses = []\n        self.d_losses = []\n        self.model_name = \"IC-GAN\"\n    \n    def train_step(self, real_imgs):\n        batch_size = real_imgs.size(0)\n        \n        # Ground truths\n        valid = torch.ones(batch_size, 1, device=device)\n        fake = torch.zeros(batch_size, 1, device=device)\n        \n        # -----------------\n        #  Train Encoder and Generator\n        # -----------------\n        self.optimizer_E.zero_grad()\n        self.optimizer_G.zero_grad()\n        \n        # Encode real images to get instance encodings\n        instance_encoding = self.encoder(real_imgs)\n        \n        # Sample noise\n        z = torch.randn(batch_size, self.latent_dim, device=device)\n        \n        # Generate a batch of images\n        gen_imgs = self.generator(z, instance_encoding)\n        \n        # Adversarial loss for the generator\n        validity, _ = self.discriminator(gen_imgs)\n        g_loss = self.adversarial_loss(validity, valid)\n        \n        g_loss.backward()\n        self.optimizer_G.step()\n        self.optimizer_E.step()\n        \n        # -----------------\n        #  Train Discriminator\n        # -----------------\n        self.optimizer_D.zero_grad()\n        \n        # Real images\n        real_validity, _ = self.discriminator(real_imgs)\n        d_real_loss = self.adversarial_loss(real_validity, valid)\n        \n        # Fake images (detached to avoid training generator)\n        fake_validity, _ = self.discriminator(gen_imgs.detach())\n        d_fake_loss = self.adversarial_loss(fake_validity, fake)\n        \n        d_loss = (d_real_loss + d_fake_loss) / 2\n        \n        d_loss.backward()\n        self.optimizer_D.step()\n        \n        return g_loss.item(), d_loss.item()\n    \n    def train(self, dataloader, num_epochs):\n        print(f\"Starting {self.model_name} Training...\")\n        \n        # Fixed noise for visualization\n        fixed_noise = torch.randn(16, self.latent_dim, device=device)\n        \n        # Get a batch of fixed real images for instance encoding\n        fixed_real_imgs = next(iter(dataloader))[0][:16].to(device)\n        fixed_instances = self.encoder(fixed_real_imgs)\n        \n        for epoch in range(num_epochs):\n            epoch_g_loss = 0\n            epoch_d_loss = 0\n            \n            for i, (real_imgs, _) in enumerate(dataloader):\n                real_imgs = real_imgs.to(device)\n                \n                # Training step\n                g_loss, d_loss = self.train_step(real_imgs)\n                \n                # Update epoch losses\n                epoch_g_loss += g_loss\n                epoch_d_loss += d_loss\n                \n                # Print status\n                if i % 50 == 0:\n                    print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n                          f\"[D loss: {d_loss:.4f}] [G loss: {g_loss:.4f}]\")\n            \n            # Save average epoch losses\n            self.g_losses.append(epoch_g_loss / len(dataloader))\n            self.d_losses.append(epoch_d_loss / len(dataloader))\n            \n            # Generate and save sample images\n            if epoch % 10 == 0 or epoch == num_epochs - 1:\n                with torch.no_grad():\n                    # Generate fake images with fixed noise and instances\n                    fake_imgs = self.generator(fixed_noise, fixed_instances).detach().cpu()\n                    \n                    # Make grid and save\n                    grid = make_grid(fake_imgs, nrow=4, normalize=True)\n                    save_image(grid, f\"icgan_samples_epoch_{epoch}.png\")\n        \n        # Save the final models\n        torch.save(self.encoder.state_dict(), \"icgan_encoder.pth\")\n        torch.save(self.generator.state_dict(), \"icgan_generator.pth\")\n        torch.save(self.discriminator.state_dict(), \"icgan_discriminator.pth\")\n        \n        return self.g_losses, self.d_losses\n    \n    def generate_fake_images(self, real_images=None, num_images=16):\n        self.encoder.eval()\n        self.generator.eval()\n        \n        with torch.no_grad():\n            # If real images are provided, use them for instance encoding\n            if real_images is not None:\n                real_images = real_images.to(device)\n                instances = self.encoder(real_images)\n            else:\n                # Use a batch from the dataloader\n                real_images = next(iter(train_loader))[0][:num_images].to(device)\n                instances = self.encoder(real_images)\n            \n            # Generate random noise\n            z = torch.randn(num_images, self.latent_dim, device=device)\n            \n            # Generate fake images\n            fake_images = self.generator(z, instances).detach().cpu()\n            \n            # Denormalize\n            fake_images = (fake_images + 1) / 2\n            \n            # Create grid of both real and fake for comparison\n            real_images = (real_images.detach().cpu() + 1) / 2\n            comparison = torch.cat([real_images, fake_images], dim=0)\n            grid = make_grid(comparison, nrow=num_images, normalize=False)\n            \n            save_image(grid, \"icgan_real_vs_fake.png\")\n            return grid\n    \n    def detect_fake_images(self, images):\n        self.discriminator.eval()\n        with torch.no_grad():\n            images = images.to(device)\n            # D returns probability that each image is real\n            scores, _ = self.discriminator(images)\n            predictions = (scores >= 0.5).float()  # 1 for real, 0 for fake\n            return scores, predictions\n\n# Initialize and train the IC-GAN model\nmodel = ICGAN(latent_dim, instance_dim, img_channels)\ng_losses, d_losses = model.train(train_loader, num_epochs)\n\n# Plot losses\nplt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(g_losses, label=\"G\")\nplt.plot(d_losses, label=\"D\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(\"icgan_loss_plot.png\")\nplt.show()\n\n# Generate fake images and compare with real images\ncomparison_grid = model.generate_fake_images()\n\n# Function to evaluate the discriminator's performance\ndef evaluate_discriminator(model, dataloader, num_batches=10):\n    model.discriminator.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for i, (real_imgs, _) in enumerate(dataloader):\n            if i >= num_batches:\n                break\n                \n            batch_size = real_imgs.size(0)\n            real_imgs = real_imgs.to(device)\n            \n            # Get instance encoding and generate fake images\n            instances = model.encoder(real_imgs)\n            z = torch.randn(batch_size, model.latent_dim, device=device)\n            fake_imgs = model.generator(z, instances)\n            \n            # Evaluate on real images (should be classified as real - 1)\n            real_preds, _ = model.discriminator(real_imgs)\n            real_correct = ((real_preds >= 0.5).float() == 1).sum().item()\n            \n            # Evaluate on fake images (should be classified as fake - 0)\n            fake_preds, _ = model.discriminator(fake_imgs)\n            fake_correct = ((fake_preds < 0.5).float() == 1).sum().item()\n            \n            # Accumulate statistics\n            correct += (real_correct + fake_correct)\n            total += (batch_size * 2)  # Both real and fake\n    \n    accuracy = 100 * correct / total\n    print(f\"Discriminator accuracy: {accuracy:.2f}%\")\n    return accuracy\n\n# Evaluate the discriminator\naccuracy = evaluate_discriminator(model, train_loader)\n\n# Now let's create a function to visualize instance-conditioned generation\ndef visualize_instance_conditioning(model, num_instances=4, num_variations=4):\n    \"\"\"Generate multiple variations from the same instance encodings\"\"\"\n    model.encoder.eval()\n    model.generator.eval()\n    \n    with torch.no_grad():\n        # Get some real images for instance encoding\n        real_imgs = next(iter(train_loader))[0][:num_instances].to(device)\n        instances = model.encoder(real_imgs)\n        \n        # Create output tensors\n        all_images = []\n        all_images.append((real_imgs.detach().cpu() + 1) / 2)  # Add real images (denormalized)\n        \n        # For each instance, generate multiple variants\n        for i in range(num_variations):\n            z = torch.randn(num_instances, model.latent_dim, device=device)\n            fake_imgs = model.generator(z, instances)\n            all_images.append((fake_imgs.detach().cpu() + 1) / 2)  # Denormalize\n        \n        # Stack all images\n        all_images = torch.cat(all_images, dim=0)\n        \n        # Create a grid\n        grid = make_grid(all_images, nrow=num_instances, normalize=False)\n        save_image(grid, \"icgan_instance_variations.png\")\n        \n        return grid\n\n# Visualize instance conditioning\ninstance_variations = visualize_instance_conditioning(model)\n\nprint(\"IC-GAN implementation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:09:56.355661Z","iopub.execute_input":"2025-04-19T15:09:56.355974Z","iopub.status.idle":"2025-04-19T15:31:02.944498Z","shell.execute_reply.started":"2025-04-19T15:09:56.355948Z","shell.execute_reply":"2025-04-19T15:31:02.943585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Path to the directory you want to zip\ndir_to_zip = '/kaggle/working/data'\n# Output zip file path\nzip_file_path = '/kaggle/working/data.zip'\n\n# Create the zip file\nshutil.make_archive(base_name=zip_file_path.replace('.zip', ''), format='zip', root_dir=dir_to_zip)\n\nprint(\"Zipping completed:\", zip_file_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:52:00.247075Z","iopub.execute_input":"2025-04-19T16:52:00.247427Z","iopub.status.idle":"2025-04-19T16:52:15.223737Z","shell.execute_reply.started":"2025-04-19T16:52:00.247395Z","shell.execute_reply":"2025-04-19T16:52:15.223057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}